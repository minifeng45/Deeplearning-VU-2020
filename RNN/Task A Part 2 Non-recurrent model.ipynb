{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Loading_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = []\n",
    "target = []\n",
    "for j in range(500):\n",
    "    tensor_x = []\n",
    "    for i in range(40):\n",
    "        tensor_x.append(torch.tensor(x_train[i+j*40], dtype=torch.long))\n",
    "    tensor_pad = pad_sequence(tensor_x, batch_first=True, padding_value=0)\n",
    "    batch.append(tensor_pad)\n",
    "    target.append(torch.tensor(y_train[0+j*40:40+j*40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_batch = []\n",
    "v_target = []\n",
    "for j in range(125):\n",
    "    tensor_x = []\n",
    "    for i in range(40):\n",
    "        tensor_x.append(torch.tensor(x_train[i+j*40], dtype=torch.long))\n",
    "    tensor_pad = pad_sequence(tensor_x, batch_first=True, padding_value=0)\n",
    "    v_batch.append(tensor_pad)\n",
    "    v_target.append(torch.tensor(y_train[0+j*40:40+j*40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Embedding(99430, 300)\n",
      "  (fc1): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 16 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.layer1 = nn.Embedding(len(i2w),300)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(300,300)  \n",
    "        self.fc2 = nn.Linear(300,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.max(F.relu(self.fc1(x)),1)\n",
    "        x = self.fc2(x.values)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(),betas=(0.9, 0.999), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7001241445541382\n",
      "0.7150899171829224\n",
      "0.6627795100212097\n",
      "0.6540238857269287\n",
      "0.6366941928863525\n",
      "0.657733142375946\n",
      "0.7591081857681274\n",
      "0.8538466691970825\n",
      "0.6231381893157959\n",
      "0.5254716277122498\n",
      "0.7184483408927917\n",
      "0.7147964239120483\n",
      "0.7689799070358276\n",
      "0.7658973932266235\n",
      "0.6440830826759338\n",
      "0.682274580001831\n",
      "0.6421079635620117\n",
      "0.6639162302017212\n",
      "0.6613119840621948\n",
      "0.675513744354248\n",
      "0.6635802388191223\n",
      "0.6465173363685608\n",
      "0.6434848308563232\n",
      "0.6351696252822876\n",
      "0.5986979007720947\n",
      "0.7097248435020447\n",
      "0.684760570526123\n",
      "0.6138460040092468\n",
      "0.6622329950332642\n",
      "0.7001656293869019\n",
      "0.6496512293815613\n",
      "0.642638087272644\n",
      "0.6129118800163269\n",
      "0.6189146041870117\n",
      "0.5985599756240845\n",
      "0.6103549003601074\n",
      "0.5963338613510132\n",
      "0.6092076897621155\n",
      "0.599199652671814\n",
      "0.5961412191390991\n",
      "0.6233580708503723\n",
      "0.6234766244888306\n",
      "0.5956727266311646\n",
      "0.5467987656593323\n",
      "0.5975139737129211\n",
      "0.6005131602287292\n",
      "0.6302342414855957\n",
      "0.5522904396057129\n",
      "0.5967479944229126\n",
      "0.6365700960159302\n",
      "0.5535088181495667\n",
      "0.5972403287887573\n",
      "0.5806043148040771\n",
      "0.5980864763259888\n",
      "0.5514668822288513\n",
      "0.5346764326095581\n",
      "0.5130951404571533\n",
      "0.5335314273834229\n",
      "0.5710135698318481\n",
      "0.49876946210861206\n",
      "0.48517823219299316\n",
      "0.535129725933075\n",
      "0.57831871509552\n",
      "0.46523866057395935\n",
      "0.5342738628387451\n",
      "0.5990228652954102\n",
      "0.5566288232803345\n",
      "0.47200822830200195\n",
      "0.5919182896614075\n",
      "0.4711594581604004\n",
      "0.5494376420974731\n",
      "0.5343249440193176\n",
      "0.4587990641593933\n",
      "0.5566186308860779\n",
      "0.47974079847335815\n",
      "0.573867917060852\n",
      "0.41023722290992737\n",
      "0.5761083364486694\n",
      "0.47226935625076294\n",
      "0.5035518407821655\n",
      "0.4359053075313568\n",
      "0.5188764333724976\n",
      "0.5942168831825256\n",
      "0.4790269732475281\n",
      "0.5675354599952698\n",
      "0.4118250906467438\n",
      "0.48387831449508667\n",
      "0.49379998445510864\n",
      "0.654950737953186\n",
      "0.4702931344509125\n",
      "0.6273377537727356\n",
      "0.42873817682266235\n",
      "0.48569798469543457\n",
      "0.6632874011993408\n",
      "0.44472113251686096\n",
      "0.5015331506729126\n",
      "0.6729532480239868\n",
      "0.49730610847473145\n",
      "0.5744460821151733\n",
      "0.3685571253299713\n",
      "0.5332592129707336\n",
      "0.5358827114105225\n",
      "0.36211657524108887\n",
      "0.6433931589126587\n",
      "0.5100582838058472\n",
      "0.6330224275588989\n",
      "0.4819425642490387\n",
      "0.515439510345459\n",
      "0.5328834652900696\n",
      "0.3744906485080719\n",
      "0.44283074140548706\n",
      "0.5126141309738159\n",
      "0.7926554679870605\n",
      "0.40046048164367676\n",
      "0.46094027161598206\n",
      "0.6349522471427917\n",
      "0.4085169732570648\n",
      "0.4313649535179138\n",
      "0.6207507848739624\n",
      "0.584326446056366\n",
      "0.47360020875930786\n",
      "0.49396219849586487\n",
      "0.5012835264205933\n",
      "0.38768982887268066\n",
      "0.6072710752487183\n",
      "0.5044599771499634\n",
      "0.46458715200424194\n",
      "0.6080821752548218\n",
      "0.5738402605056763\n",
      "0.489184707403183\n",
      "0.3913119435310364\n",
      "0.5559252500534058\n",
      "0.5081506371498108\n",
      "0.4629141688346863\n",
      "0.44704094529151917\n",
      "0.43950748443603516\n",
      "0.6551719307899475\n",
      "0.4003244936466217\n",
      "0.4671005308628082\n",
      "0.5609778165817261\n",
      "0.6206990480422974\n",
      "0.41121214628219604\n",
      "0.48526468873023987\n",
      "0.5624109506607056\n",
      "0.40147584676742554\n",
      "0.48574525117874146\n",
      "0.5243669748306274\n",
      "0.5312861204147339\n",
      "0.4647729992866516\n",
      "0.49899929761886597\n",
      "0.4951677918434143\n",
      "0.37690967321395874\n",
      "0.5341389179229736\n",
      "0.4712231755256653\n",
      "0.3331799805164337\n",
      "0.5194445252418518\n",
      "0.5875732898712158\n",
      "0.5437779426574707\n",
      "0.458737313747406\n",
      "0.5575474500656128\n",
      "0.5355747938156128\n",
      "0.5394951105117798\n",
      "0.5180791020393372\n",
      "0.37689071893692017\n",
      "0.5111583471298218\n",
      "0.585262656211853\n",
      "0.3678005039691925\n",
      "0.4177519381046295\n",
      "0.5527969598770142\n",
      "0.39892715215682983\n",
      "0.46030664443969727\n",
      "0.6114362478256226\n",
      "0.42337241768836975\n",
      "0.49503105878829956\n",
      "0.38383617997169495\n",
      "0.4145713448524475\n",
      "0.5854922533035278\n",
      "0.42076414823532104\n",
      "0.5076390504837036\n",
      "0.6580855250358582\n",
      "0.4227202832698822\n",
      "0.522235095500946\n",
      "0.3828602731227875\n",
      "0.49511757493019104\n",
      "0.49172458052635193\n",
      "0.47417545318603516\n",
      "0.5535210967063904\n",
      "0.5511317253112793\n",
      "0.42461585998535156\n",
      "0.5333472490310669\n",
      "0.3694809675216675\n",
      "0.5176132917404175\n",
      "0.5003795623779297\n",
      "0.4356216788291931\n",
      "0.45004725456237793\n",
      "0.33494487404823303\n",
      "0.4736076891422272\n",
      "0.5606627464294434\n",
      "0.4254825711250305\n",
      "0.527419924736023\n",
      "0.5750694870948792\n",
      "0.4062957167625427\n",
      "0.5487948656082153\n",
      "0.434001624584198\n",
      "0.5908398032188416\n",
      "0.3524623513221741\n",
      "0.45753374695777893\n",
      "0.5355270504951477\n",
      "0.4012826085090637\n",
      "0.5108946561813354\n",
      "0.3445478677749634\n",
      "0.49918633699417114\n",
      "0.5651158094406128\n",
      "0.4380861222743988\n",
      "0.502555251121521\n",
      "0.3779258131980896\n",
      "0.4806988835334778\n",
      "0.41939377784729004\n",
      "0.5328139066696167\n",
      "0.35703688859939575\n",
      "0.4633745551109314\n",
      "0.3330622911453247\n",
      "0.44557809829711914\n",
      "0.34740933775901794\n",
      "0.4187685549259186\n",
      "0.5309097766876221\n",
      "0.4021163880825043\n",
      "0.6524449586868286\n",
      "0.43639978766441345\n",
      "0.5333834886550903\n",
      "0.3649497628211975\n",
      "0.47324252128601074\n",
      "0.3772457540035248\n",
      "0.5334920883178711\n",
      "0.39817380905151367\n",
      "0.6158278584480286\n",
      "0.4125930368900299\n",
      "0.5578622817993164\n",
      "0.4201721251010895\n",
      "0.5097358822822571\n",
      "0.37639015913009644\n",
      "0.5172743201255798\n",
      "0.41501837968826294\n",
      "0.5146161913871765\n",
      "0.44343096017837524\n",
      "0.5149911046028137\n",
      "0.4173138737678528\n",
      "0.4560314118862152\n",
      "0.4506170153617859\n",
      "0.382524311542511\n",
      "0.5040663480758667\n",
      "0.3425305485725403\n",
      "0.5317404866218567\n",
      "0.3523593544960022\n",
      "0.684560239315033\n",
      "0.4246238172054291\n",
      "0.35777097940444946\n",
      "0.4473239779472351\n",
      "0.382613867521286\n",
      "0.7080394625663757\n",
      "0.4780079424381256\n",
      "0.33489564061164856\n",
      "0.4974878430366516\n",
      "0.34540656208992004\n",
      "0.5585207939147949\n",
      "0.4319135546684265\n",
      "0.3620656132698059\n",
      "0.5473548173904419\n",
      "0.42778557538986206\n",
      "0.4968143105506897\n",
      "0.46369943022727966\n",
      "0.3898046910762787\n",
      "0.4561547636985779\n",
      "0.4858407974243164\n",
      "0.43090683221817017\n",
      "0.5722082257270813\n",
      "0.44072169065475464\n",
      "0.3650291860103607\n",
      "0.4186040759086609\n",
      "0.511100709438324\n",
      "0.3778466284275055\n",
      "0.3772459626197815\n",
      "0.4980221688747406\n",
      "0.35166293382644653\n",
      "0.3517071604728699\n",
      "0.500468373298645\n",
      "0.3472908139228821\n",
      "0.4787971079349518\n",
      "0.5533467531204224\n",
      "0.3792451322078705\n",
      "0.35471311211586\n",
      "0.3996336758136749\n",
      "0.5456640720367432\n",
      "0.4039943218231201\n",
      "0.37954574823379517\n",
      "0.4829469621181488\n",
      "0.4041483402252197\n",
      "0.4067433476448059\n",
      "0.3623882532119751\n",
      "0.4747233986854553\n",
      "0.5331311821937561\n",
      "0.39461931586265564\n",
      "0.35515546798706055\n",
      "0.6024155616760254\n",
      "0.4890843331813812\n",
      "0.39247778058052063\n",
      "0.3323524296283722\n",
      "0.505179762840271\n",
      "0.5115796327590942\n",
      "0.5682929754257202\n",
      "0.5301156640052795\n",
      "0.48948541283607483\n",
      "0.4222572445869446\n",
      "0.3910284638404846\n",
      "0.4103756844997406\n",
      "0.33253419399261475\n",
      "0.3891007900238037\n",
      "0.49989670515060425\n",
      "0.6075632572174072\n",
      "0.5093320608139038\n",
      "0.46184295415878296\n",
      "0.5997010469436646\n",
      "0.5205485224723816\n",
      "0.46899405121803284\n",
      "0.42374294996261597\n",
      "0.4654526114463806\n",
      "0.5955237150192261\n",
      "0.5854769945144653\n",
      "0.5541480779647827\n",
      "0.4498559534549713\n",
      "0.39793768525123596\n",
      "0.3490367531776428\n",
      "0.4424513280391693\n",
      "0.4022858142852783\n",
      "0.4029865264892578\n",
      "0.3686318099498749\n",
      "0.4683104157447815\n",
      "0.3948032259941101\n",
      "0.43641477823257446\n",
      "0.4508439600467682\n",
      "0.4534265398979187\n",
      "0.47819089889526367\n",
      "0.5454962253570557\n",
      "0.5346094369888306\n",
      "0.47225576639175415\n",
      "0.39775723218917847\n",
      "0.5139750242233276\n",
      "0.6204648017883301\n",
      "0.443021684885025\n",
      "0.5320515036582947\n",
      "0.4562302529811859\n",
      "0.3816531002521515\n",
      "0.38960447907447815\n",
      "0.4523444175720215\n",
      "0.6340689063072205\n",
      "0.4394668638706207\n",
      "0.419259250164032\n",
      "0.40413540601730347\n",
      "0.48580923676490784\n",
      "0.4032208025455475\n",
      "0.4552231729030609\n",
      "0.4763454496860504\n",
      "0.522520124912262\n",
      "0.3400878608226776\n",
      "0.45169052481651306\n",
      "0.4316589832305908\n",
      "0.45089268684387207\n",
      "0.4932367205619812\n",
      "0.47508102655410767\n",
      "0.458395779132843\n",
      "0.5401926040649414\n",
      "0.48811641335487366\n",
      "0.4773477613925934\n",
      "0.44566279649734497\n",
      "0.44239792227745056\n",
      "0.4825894236564636\n",
      "0.4469287395477295\n",
      "0.45189934968948364\n",
      "0.42727595567703247\n",
      "0.4356275498867035\n",
      "0.4242929518222809\n",
      "0.5245482921600342\n",
      "0.5628407597541809\n",
      "0.41953882575035095\n",
      "0.5559406280517578\n",
      "0.48665300011634827\n",
      "0.4586044251918793\n",
      "0.5321210026741028\n",
      "0.4043117165565491\n",
      "0.5290811061859131\n",
      "0.4772745966911316\n",
      "0.4794904291629791\n",
      "0.46216750144958496\n",
      "0.5337945818901062\n",
      "0.4485708177089691\n",
      "0.4311673641204834\n",
      "0.425372838973999\n",
      "0.6357841491699219\n",
      "0.507170557975769\n",
      "0.5246535539627075\n",
      "0.49065151810646057\n",
      "0.44760775566101074\n",
      "0.4703696370124817\n",
      "0.4194919466972351\n",
      "0.623115062713623\n",
      "0.5092352628707886\n",
      "0.43678703904151917\n",
      "0.5456960201263428\n",
      "0.48999571800231934\n",
      "0.39546263217926025\n",
      "0.5462773442268372\n",
      "0.4553975462913513\n",
      "0.43100833892822266\n",
      "0.5264592170715332\n",
      "0.46143966913223267\n",
      "0.5430563688278198\n",
      "0.5077979564666748\n",
      "0.49069729447364807\n",
      "0.48826971650123596\n",
      "0.44501981139183044\n",
      "0.4980947971343994\n",
      "0.4687127470970154\n",
      "0.5102726817131042\n",
      "0.4490155279636383\n",
      "0.4549858570098877\n",
      "0.5042805671691895\n",
      "0.4287848472595215\n",
      "0.4417092800140381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5614744424819946\n",
      "0.46035805344581604\n",
      "0.49954819679260254\n",
      "0.47527962923049927\n",
      "0.5126016139984131\n",
      "0.4267142415046692\n",
      "0.4807652533054352\n",
      "0.536629855632782\n",
      "0.4788537919521332\n",
      "0.5083392858505249\n",
      "0.47262391448020935\n",
      "0.4723193645477295\n",
      "0.49001622200012207\n",
      "0.43655553460121155\n",
      "0.5728859305381775\n",
      "0.5246618390083313\n",
      "0.5135493874549866\n",
      "0.533228874206543\n",
      "0.39447423815727234\n",
      "0.546818196773529\n",
      "0.5578999519348145\n",
      "0.5708019137382507\n",
      "0.5053207874298096\n",
      "0.4773697257041931\n",
      "0.4232805371284485\n",
      "0.5452023148536682\n",
      "0.5146154761314392\n",
      "0.4525693953037262\n",
      "0.45708179473876953\n",
      "0.41224271059036255\n",
      "0.4760531783103943\n",
      "0.46670103073120117\n",
      "0.4741618037223816\n",
      "0.52752286195755\n",
      "0.4545159339904785\n",
      "0.4910198748111725\n",
      "0.44732117652893066\n",
      "0.4337555468082428\n",
      "0.49452081322669983\n",
      "0.48896822333335876\n",
      "0.47923341393470764\n",
      "0.4217609465122223\n",
      "0.39030465483665466\n",
      "0.47402334213256836\n",
      "0.4940546154975891\n",
      "0.45064935088157654\n",
      "0.4356105327606201\n",
      "0.43621787428855896\n",
      "0.386301189661026\n",
      "0.47963976860046387\n",
      "0.49094367027282715\n",
      "0.5111312866210938\n",
      "0.5850538015365601\n",
      "0.4040408134460449\n",
      "0.5028997659683228\n",
      "0.45514464378356934\n",
      "0.4339156746864319\n",
      "0.49470287561416626\n",
      "0.4779109060764313\n",
      "0.4947761595249176\n",
      "0.4329310953617096\n",
      "0.48456135392189026\n",
      "0.49136823415756226\n",
      "0.46409472823143005\n",
      "0.5005788803100586\n",
      "0.4314758777618408\n",
      "0.3974970281124115\n",
      "0.5057257413864136\n",
      "0.4343460500240326\n",
      "0.5535281896591187\n",
      "0.45913419127464294\n",
      "0.44603127241134644\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1): \n",
    "    for j in range(500):\n",
    "        outputs = net(batch[j])\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.nn.functional.cross_entropy(F.softmax(outputs,1),target[j])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 5000 test images: 87 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for j in range(125):\n",
    "        outputs = net(v_batch[j])\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += v_target[j].size(0)\n",
    "        correct += (predicted == v_target[j]).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 5000 test: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
