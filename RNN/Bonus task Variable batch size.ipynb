{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = load_imdb(final=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a maximum number of tokens\n",
    "k = 0\n",
    "for i in range(20000):\n",
    "    k += len(x_train[i])\n",
    "token_perbatch = k/500\n",
    "# generate batches\n",
    "i = 0\n",
    "batch = []\n",
    "target = []\n",
    "batch_number = 0\n",
    "while i < 20000: # loop within training data\n",
    "    token_number = 0\n",
    "    tensor_x = []\n",
    "    tensor_y = []\n",
    "# Generate batch within a maximum number of tokens & loop within training data\n",
    "    while (token_number < token_perbatch) & (i <20000): \n",
    "        tensor_x.append(torch.tensor(x_train[i], dtype=torch.long))\n",
    "        token_number += len(x_train[i])\n",
    "        tensor_y.append(y_train[i])\n",
    "        i += 1\n",
    "    tensor_pad = pad_sequence(tensor_x, batch_first=True, padding_value=0)\n",
    "    batch.append(tensor_pad)\n",
    "    target.append(torch.tensor(tensor_y, dtype=torch.long))\n",
    "    batch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a maximum number of tokens\n",
    "k = 0\n",
    "for i in range(5000):\n",
    "    k += len(x_val[i])\n",
    "token_perbatch = k/500\n",
    "# generate batches\n",
    "i = 0\n",
    "v_batch = []\n",
    "v_target = []\n",
    "v_batch_number = 0\n",
    "while i < 5000: # loop within training data\n",
    "    token_number = 0\n",
    "    tensor_x = []\n",
    "    tensor_y = []\n",
    "# Generate batch within a maximum number of tokens & loop within training data\n",
    "    while (token_number < token_perbatch) & (i <5000): \n",
    "        tensor_x.append(torch.tensor(x_val[i], dtype=torch.long))\n",
    "        token_number += len(x_val[i])\n",
    "        tensor_y.append(y_val[i])\n",
    "        i += 1\n",
    "    tensor_pad = pad_sequence(tensor_x, batch_first=True, padding_value=0)\n",
    "    v_batch.append(tensor_pad)\n",
    "    v_target.append(torch.tensor(tensor_y, dtype=torch.long))\n",
    "    v_batch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Embedding(99430, 300)\n",
      "  (fc1): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 16 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.layer1 = nn.Embedding(len(i2w),300)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(300,300)  \n",
    "        self.fc2 = nn.Linear(300,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.max(F.relu(self.fc1(x)),1)\n",
    "        x = self.fc2(x.values)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(),betas=(0.9, 0.999), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7028366923332214\n",
      "0.6721299290657043\n",
      "0.6936236023902893\n",
      "0.6816436648368835\n",
      "0.6804319620132446\n",
      "0.6437385082244873\n",
      "0.6807283759117126\n",
      "0.6724631786346436\n",
      "0.6500222682952881\n",
      "0.6572632789611816\n",
      "0.6541025042533875\n",
      "0.6454006433486938\n",
      "0.6652977466583252\n",
      "0.6442278623580933\n",
      "0.6335259079933167\n",
      "0.6338663101196289\n",
      "0.6492671370506287\n",
      "0.6158459186553955\n",
      "0.6196764707565308\n",
      "0.6595302820205688\n",
      "0.6278300285339355\n",
      "0.6226454377174377\n",
      "0.6307505369186401\n",
      "0.6310650706291199\n",
      "0.5991341471672058\n",
      "0.5844296813011169\n",
      "0.6189343333244324\n",
      "0.6082825064659119\n",
      "0.5960353016853333\n",
      "0.6332985758781433\n",
      "0.581642746925354\n",
      "0.578914999961853\n",
      "0.596298336982727\n",
      "0.5860658884048462\n",
      "0.5939655303955078\n",
      "0.5807291269302368\n",
      "0.5879064798355103\n",
      "0.5610102415084839\n",
      "0.6360515356063843\n",
      "0.517217218875885\n",
      "0.6883792877197266\n",
      "0.4967830777168274\n",
      "0.5647851824760437\n",
      "0.5592250227928162\n",
      "0.5597575902938843\n",
      "0.5338048934936523\n",
      "0.614253580570221\n",
      "0.4949875473976135\n",
      "0.6175767779350281\n",
      "0.5285652875900269\n",
      "0.5175122022628784\n",
      "0.5595317482948303\n",
      "0.5883888602256775\n",
      "0.5745676159858704\n",
      "0.4416038691997528\n",
      "0.5900554656982422\n",
      "0.496193528175354\n",
      "0.6063239574432373\n",
      "0.4763922691345215\n",
      "0.5488768815994263\n",
      "0.4659067690372467\n",
      "0.5572687387466431\n",
      "0.47464847564697266\n",
      "0.5404900312423706\n",
      "0.43788790702819824\n",
      "0.5285589098930359\n",
      "0.46271204948425293\n",
      "0.5907548069953918\n",
      "0.4484142065048218\n",
      "0.6393724679946899\n",
      "0.5093073844909668\n",
      "0.47618821263313293\n",
      "0.5455017685890198\n",
      "0.4430229961872101\n",
      "0.5469123125076294\n",
      "0.5474376082420349\n",
      "0.4326653778553009\n",
      "0.576401948928833\n",
      "0.47493287920951843\n",
      "0.42729297280311584\n",
      "0.5471056699752808\n",
      "0.43842029571533203\n",
      "0.5949279069900513\n",
      "0.4646141827106476\n",
      "0.43221375346183777\n",
      "0.61191725730896\n",
      "0.423067182302475\n",
      "0.577355682849884\n",
      "0.5006139874458313\n",
      "0.4049578607082367\n",
      "0.5131195187568665\n",
      "0.4430558681488037\n",
      "0.4527595341205597\n",
      "0.5116686820983887\n",
      "0.4003102779388428\n",
      "0.5618090629577637\n",
      "0.5368190407752991\n",
      "0.390412300825119\n",
      "0.5127904415130615\n",
      "0.40293532609939575\n",
      "0.6761040687561035\n",
      "0.5192805528640747\n",
      "0.3924170434474945\n",
      "0.5581306219100952\n",
      "0.4376155734062195\n",
      "0.42273208498954773\n",
      "0.4801015555858612\n",
      "0.4911734461784363\n",
      "0.40003007650375366\n",
      "0.3978026509284973\n",
      "0.47513318061828613\n",
      "0.3883645534515381\n",
      "0.3909103274345398\n",
      "0.5341020226478577\n",
      "0.3877081274986267\n",
      "0.48679932951927185\n",
      "0.5597639083862305\n",
      "0.5251960158348083\n",
      "0.3964255750179291\n",
      "0.4521775245666504\n",
      "0.5143678188323975\n",
      "0.3700005114078522\n",
      "0.3517000675201416\n",
      "0.5101136565208435\n",
      "0.39736607670783997\n",
      "0.3620271384716034\n",
      "0.5841573476791382\n",
      "0.5249175429344177\n",
      "0.399723619222641\n",
      "0.399152934551239\n",
      "0.4304738938808441\n",
      "0.5913547873497009\n",
      "0.4711225926876068\n",
      "0.3683554232120514\n",
      "0.4369966983795166\n",
      "0.39141249656677246\n",
      "0.5531520843505859\n",
      "0.3884248733520508\n",
      "0.35271012783050537\n",
      "0.607964277267456\n",
      "0.6200823783874512\n",
      "0.5109449028968811\n",
      "0.369060754776001\n",
      "0.3684784471988678\n",
      "0.5391363501548767\n",
      "0.633573055267334\n",
      "0.5409510731697083\n",
      "0.47201400995254517\n",
      "0.3898394703865051\n",
      "0.5033414363861084\n",
      "0.4464800953865051\n",
      "0.5555601119995117\n",
      "0.5437524914741516\n",
      "0.45437338948249817\n",
      "0.387858510017395\n",
      "0.35333526134490967\n",
      "0.6750026345252991\n",
      "0.561059296131134\n",
      "0.5026254653930664\n",
      "0.42599722743034363\n",
      "0.41265228390693665\n",
      "0.37465700507164\n",
      "0.4708866477012634\n",
      "0.6757297515869141\n",
      "0.48000234365463257\n",
      "0.45976659655570984\n",
      "0.43989720940589905\n",
      "0.42491379380226135\n",
      "0.3680301010608673\n",
      "0.4544401466846466\n",
      "0.46036437153816223\n",
      "0.5971246361732483\n",
      "0.49091628193855286\n",
      "0.4370165467262268\n",
      "0.37426114082336426\n",
      "0.4312543570995331\n",
      "0.41792505979537964\n",
      "0.43598511815071106\n",
      "0.39357852935791016\n",
      "0.36177074909210205\n",
      "0.37915438413619995\n",
      "0.41366758942604065\n",
      "0.3437577784061432\n",
      "0.3493439853191376\n",
      "0.45864421129226685\n",
      "0.5132324695587158\n",
      "0.35971197485923767\n",
      "0.5317009687423706\n",
      "0.368056982755661\n",
      "0.3785228133201599\n",
      "0.4203203320503235\n",
      "0.5306292176246643\n",
      "0.46237489581108093\n",
      "0.514454185962677\n",
      "0.3882288634777069\n",
      "0.3663850724697113\n",
      "0.40967774391174316\n",
      "0.4514143764972687\n",
      "0.5621601343154907\n",
      "0.4785740375518799\n",
      "0.40288084745407104\n",
      "0.4149671494960785\n",
      "0.40982794761657715\n",
      "0.38443148136138916\n",
      "0.4806889295578003\n",
      "0.41241753101348877\n",
      "0.41563743352890015\n",
      "0.3886757493019104\n",
      "0.4696049690246582\n",
      "0.6027492880821228\n",
      "0.5864126682281494\n",
      "0.5956165194511414\n",
      "0.4344659149646759\n",
      "0.35407522320747375\n",
      "0.41932013630867004\n",
      "0.3574257791042328\n",
      "0.4394073486328125\n",
      "0.3826751708984375\n",
      "0.4127313494682312\n",
      "0.41345205903053284\n",
      "0.34484371542930603\n",
      "0.4481760859489441\n",
      "0.4485231935977936\n",
      "0.5316751599311829\n",
      "0.4902550280094147\n",
      "0.798897922039032\n",
      "0.6619585156440735\n",
      "0.5915307998657227\n",
      "0.4337315857410431\n",
      "0.5508529543876648\n",
      "0.7398346066474915\n",
      "0.3544035851955414\n",
      "0.6230528950691223\n",
      "0.567617654800415\n",
      "0.44509512186050415\n",
      "0.7056028246879578\n",
      "0.503115177154541\n",
      "0.45727014541625977\n",
      "0.49523505568504333\n",
      "0.46619197726249695\n",
      "0.42058607935905457\n",
      "0.4233372211456299\n",
      "0.4329376220703125\n",
      "0.4681035578250885\n",
      "0.41260990500450134\n",
      "0.4625183939933777\n",
      "0.6539005637168884\n",
      "0.39112532138824463\n",
      "0.5296571850776672\n",
      "0.4233717918395996\n",
      "0.4930594563484192\n",
      "0.37212762236595154\n",
      "0.40542852878570557\n",
      "0.44216397404670715\n",
      "0.4571191668510437\n",
      "0.4823019802570343\n",
      "0.5356587171554565\n",
      "0.3733406662940979\n",
      "0.498492032289505\n",
      "0.5512075424194336\n",
      "0.6578996777534485\n",
      "0.4871053397655487\n",
      "0.6999543905258179\n",
      "0.558760404586792\n",
      "0.5298072099685669\n",
      "0.4668044447898865\n",
      "0.48753002285957336\n",
      "0.4514671564102173\n",
      "0.532340407371521\n",
      "0.4037180542945862\n",
      "0.6927155256271362\n",
      "0.5569629669189453\n",
      "0.48624879121780396\n",
      "0.7627761960029602\n",
      "0.4225153923034668\n",
      "0.41668522357940674\n",
      "0.4386146366596222\n",
      "0.4465762972831726\n",
      "0.39802125096321106\n",
      "0.49150118231773376\n",
      "0.3689832091331482\n",
      "0.49327588081359863\n",
      "0.5451978445053101\n",
      "0.5610039234161377\n",
      "0.4335925877094269\n",
      "0.47068434953689575\n",
      "0.45965370535850525\n",
      "0.619836688041687\n",
      "0.4661828875541687\n",
      "0.48716068267822266\n",
      "0.4220038950443268\n",
      "0.5195426344871521\n",
      "0.5811103582382202\n",
      "0.6723037958145142\n",
      "0.43122559785842896\n",
      "0.467094361782074\n",
      "0.46930447220802307\n",
      "0.4212157726287842\n",
      "0.4480592906475067\n",
      "0.5441876649856567\n",
      "0.5555345416069031\n",
      "0.4350535571575165\n",
      "0.4599975347518921\n",
      "0.5082439184188843\n",
      "0.47015857696533203\n",
      "0.5173876285552979\n",
      "0.4155648648738861\n",
      "0.42861446738243103\n",
      "0.4829709231853485\n",
      "0.4388617277145386\n",
      "0.49668270349502563\n",
      "0.41031646728515625\n",
      "0.39803481101989746\n",
      "0.5166563987731934\n",
      "0.4865339994430542\n",
      "0.5367437601089478\n",
      "0.39553776383399963\n",
      "0.3784330487251282\n",
      "0.5574245452880859\n",
      "0.573192834854126\n",
      "0.4115375578403473\n",
      "0.4844162166118622\n",
      "0.4742920398712158\n",
      "0.5450223088264465\n",
      "0.4395543038845062\n",
      "0.46681034564971924\n",
      "0.5113937258720398\n",
      "0.4351198971271515\n",
      "0.37780991196632385\n",
      "0.556381106376648\n",
      "0.5456706285476685\n",
      "0.41758036613464355\n",
      "0.45477092266082764\n",
      "0.5499579906463623\n",
      "0.5170285105705261\n",
      "0.44949254393577576\n",
      "0.48323366045951843\n",
      "0.5597004890441895\n",
      "0.5299991965293884\n",
      "0.5268869400024414\n",
      "0.4941994249820709\n",
      "0.4456643760204315\n",
      "0.5177881717681885\n",
      "0.5193307399749756\n",
      "0.598653256893158\n",
      "0.5871686339378357\n",
      "0.4264633357524872\n",
      "0.6504793167114258\n",
      "0.40899381041526794\n",
      "0.4582560658454895\n",
      "0.4103431701660156\n",
      "0.5822571516036987\n",
      "0.5061339139938354\n",
      "0.4684833884239197\n",
      "0.6317883729934692\n",
      "0.7861289978027344\n",
      "0.45442917943000793\n",
      "0.4285441040992737\n",
      "0.5059699416160583\n",
      "0.5048102736473083\n",
      "0.38741326332092285\n",
      "0.5488277673721313\n",
      "0.48087435960769653\n",
      "0.5155498385429382\n",
      "0.5422431826591492\n",
      "0.5195294618606567\n",
      "0.4438440799713135\n",
      "0.4425289034843445\n",
      "0.48260465264320374\n",
      "0.4465219974517822\n",
      "0.5481682419776917\n",
      "0.4214157462120056\n",
      "0.4276905059814453\n",
      "0.49480265378952026\n",
      "0.4391195774078369\n",
      "0.5489176511764526\n",
      "0.46929702162742615\n",
      "0.4146815240383148\n",
      "0.4877558648586273\n",
      "0.62966388463974\n",
      "0.352163165807724\n",
      "0.446389764547348\n",
      "0.44376784563064575\n",
      "0.5365250110626221\n",
      "0.507202684879303\n",
      "0.48996835947036743\n",
      "0.4518018960952759\n",
      "0.44709765911102295\n",
      "0.4248587489128113\n",
      "0.44895702600479126\n",
      "0.5418776273727417\n",
      "0.3905833959579468\n",
      "0.4814338684082031\n",
      "0.4780237376689911\n",
      "0.4648343622684479\n",
      "0.5282321572303772\n",
      "0.44605767726898193\n",
      "0.4629726707935333\n",
      "0.34038135409355164\n",
      "0.46630823612213135\n",
      "0.37191396951675415\n",
      "0.4106871783733368\n",
      "0.5119414329528809\n",
      "0.3892673850059509\n",
      "0.5418675541877747\n",
      "0.4300557076931\n",
      "0.3960980176925659\n",
      "0.4909915328025818\n",
      "0.4918602705001831\n",
      "0.443335622549057\n",
      "0.38752952218055725\n",
      "0.4354817569255829\n",
      "0.3510138690471649\n",
      "0.3908134698867798\n",
      "0.4567689895629883\n",
      "0.373482346534729\n",
      "0.4400656819343567\n",
      "0.5023298263549805\n",
      "0.4555097818374634\n",
      "0.4104081988334656\n",
      "0.5535765886306763\n",
      "0.5041749477386475\n",
      "0.5402549505233765\n",
      "0.4568450152873993\n",
      "0.6119157075881958\n",
      "0.5073039531707764\n",
      "0.4447968602180481\n",
      "0.40880125761032104\n",
      "0.4861676096916199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39734116196632385\n",
      "0.5404792428016663\n",
      "0.5739432573318481\n",
      "0.4417877197265625\n",
      "0.4462341368198395\n",
      "0.39978542923927307\n",
      "0.35246890783309937\n",
      "0.5008898973464966\n",
      "0.4136865735054016\n",
      "0.4144027531147003\n",
      "0.5057533979415894\n",
      "0.5483372807502747\n",
      "0.5034946799278259\n",
      "0.5516241192817688\n",
      "0.6191729307174683\n",
      "0.3816699683666229\n",
      "0.4795386493206024\n",
      "0.5313207507133484\n",
      "0.3596089780330658\n",
      "0.5396372675895691\n",
      "0.40056514739990234\n",
      "0.49908336997032166\n",
      "0.5205144286155701\n",
      "0.4412287175655365\n",
      "0.40901848673820496\n",
      "0.4859267771244049\n",
      "0.5963524580001831\n",
      "0.4916599988937378\n",
      "0.4983563721179962\n",
      "0.4474765360355377\n",
      "0.37000465393066406\n",
      "0.5182873010635376\n",
      "0.3856698274612427\n",
      "0.4686160087585449\n",
      "0.4523746967315674\n",
      "0.39500805735588074\n",
      "0.5110557675361633\n",
      "0.39143964648246765\n",
      "0.5321423411369324\n",
      "0.40006494522094727\n",
      "0.35597267746925354\n",
      "0.4355586767196655\n",
      "0.5129756331443787\n",
      "0.41239988803863525\n",
      "0.4611127972602844\n",
      "0.4041505753993988\n",
      "0.5435357093811035\n",
      "0.6165508031845093\n",
      "0.5161910057067871\n",
      "0.5539754629135132\n",
      "0.6547929644584656\n",
      "0.47829562425613403\n",
      "0.5435419082641602\n",
      "0.3603249192237854\n",
      "0.7563742995262146\n",
      "0.6123565435409546\n",
      "0.5367323160171509\n",
      "0.6097356081008911\n",
      "0.48495081067085266\n",
      "0.4169390797615051\n",
      "0.331226646900177\n",
      "0.3452903926372528\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1): \n",
    "    for j in range(batch_number):\n",
    "        outputs = net(batch[j])\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.nn.functional.cross_entropy(F.softmax(outputs,1),target[j])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 5000 test: 84 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for j in range(v_batch_number):\n",
    "        outputs = net(v_batch[j])\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += v_target[j].size(0)\n",
    "        correct += (predicted == v_target[j]).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 5000 test: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
